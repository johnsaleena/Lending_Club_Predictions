// Group Members: Jared Kacso, Sidney Johnson & Saleena John 

import numpy as np
import scipy as sp
import pandas as pd
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns

// Pandas options
pd.set_option('display.max_colwidth', 1000, 'display.max_rows', None, 'display.max_columns', None)

// Plotting options
%matplotlib inline
mpl.style.use('ggplot')
sns.set(style='whitegrid')


import os
os.chdir('/Users/saleenajohn/Downloads/DataMining/PROJECT2')

//Read the data into a pandas dataframe:
//loans = pd.read_csv('../input/accepted_2007_to_2018Q2.csv.gz', compression='gzip', low_memory=True)
//loans = pd.read_csv('../input/accepted-200000.csv')
//loans = pd.read_csv('accepted_2007_to_2018Q4.csv') 
loans = pd.read_csv('accepted-200000.csv')

//Check basic dataframe info:
loans.info()

//There are 1.6 million rows and 150 variables. The size of the dataset is 1.8 GB.
//5 randomly selected rows. Each row corresponds to a single loan.
loans.sample(5)

//3. Response Variable
//We're going to try to predict the loan_status variable.
loans['loan_status'].value_counts(dropna=False)

//We're going to try to learn differences in the features between completed loans that have been fully paid or charged off. 
//We won't consider loans that are current, don't meet the credit policy, defaulted, or have a missing status. 
//So we only keep the loans with status "Fully Paid" or "Charged Off."
help(loans.loc)
loans = loans.loc[loans['loan_status'].isin(['Fully Paid', 'Charged Off'])]

//How many loans remain in the dataset?
loans.shape

//There are 814,986 loans remaining. Check that the statuses are as expected:
loans['loan_status'].value_counts(dropna=False)

//Status counts as percentages:
loans['loan_status'].value_counts(normalize=True, dropna=False)

//4. Limit the Feature Space
//The full dataset has 150 features for each loan. We'll select features in two steps:
//Drop features with more than 30% of their data missing.
//Of the remaining features, choose only those that would be available to an investor before deciding to fund the loan.

//4.1 Drop features missing more than 30% data
//First calculate the percentage of missing data for each feature:
missing_fractions = loans.isnull().mean().sort_values(ascending=False)

//Top 10 features missing the most data:
missing_fractions.head(10)

//Let's visualize the distribution of missing data percentages:
plt.figure(figsize=(6,3), dpi=90)
missing_fractions.plot.hist(bins=20)
plt.title('Histogram of Feature Incompleteness')
plt.xlabel('Fraction of data missing')
plt.ylabel('Feature count')

drop_list = sorted(list(missing_fractions[missing_fractions > 0.3].index))
print(drop_list)
len(drop_list)

//Drop these features:
loans.drop(labels=drop_list, axis=1, inplace=True)
loans.shape

//4.2 Only keep loan features known to potential investors
//We examine the LendingClub website and Data Dictionary to determine which features would have been available to potential investors. Here's the list of features we currently have, in alphabetical order:
print(sorted(loans.columns))

//For each of these features, we check the description in the Data Dictionary and only keep the features that would have been available to investors considering an investment in the loan. These include features in the loan application, and any features added by LendingClub when the loan listing was accepted, such as the loan grade and interest rate.
keep_list = ['addr_state', 'annual_inc', 'application_type', 'dti', 'earliest_cr_line', 'emp_length', 'emp_title', 'fico_range_high', 'fico_range_low', 'grade', 'home_ownership', 'id', 'initial_list_status', 'installment', 'int_rate', 'issue_d', 'loan_amnt', 'loan_status', 'mort_acc', 'open_acc', 'pub_rec', 'pub_rec_bankruptcies', 'purpose', 'revol_bal', 'revol_util', 'sub_grade', 'term', 'title', 'total_acc', 'verification_status', 'zip_code']
len(keep_list)

//The list of features to drop is any feature not in keep_list:
drop_list = [col for col in loans.columns if col not in keep_list]
print(drop_list)

len(drop_list)
//Drop these features:
loans.drop(labels=drop_list, axis=1, inplace=True)
loans.shape

//5. Pre-processing and Exploratory Analysis
//We'll inspect each feature individually, and do the following:
//Drop the feature if it is not useful for predicting the loan status.
//View summary statistics and visualize the data, plotting against the loan status.
//Modify the feature to make it useful for modeling, if necessary.

//We define a function for plotting a variable and comparing with the loan status:
def plot_var(col_name, full_name, continuous):
    """
    Visualize a variable with and without faceting on the loan status.
    - col_name is the variable name in the dataframe
    - full_name is the full variable name
    - continuous is True if the variable is continuous, False otherwise
    """
    f, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(12,3), dpi=90)
    
    # Plot without loan status
    if continuous:
        sns.distplot(loans.loc[loans[col_name].notnull(), col_name], kde=False, ax=ax1)
    else:
        sns.countplot(loans[col_name], order=sorted(loans[col_name].unique()), color='#5975A4', saturation=1, ax=ax1)
    ax1.set_xlabel(full_name)
    ax1.set_ylabel('Count')
    ax1.set_title(full_name)

    # Plot with loan status
    if continuous:
        sns.boxplot(x=col_name, y='loan_status', data=loans, ax=ax2)
        ax2.set_ylabel('')
        ax2.set_title(full_name + ' by Loan Status')
    else:
        charge_off_rates = loans.groupby(col_name)['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']
        sns.barplot(x=charge_off_rates.index, y=charge_off_rates.values, color='#5975A4', saturation=1, ax=ax2)
        ax2.set_ylabel('Fraction of Loans Charged-off')
        ax2.set_title('Charge-off Rate by ' + full_name)
    ax2.set_xlabel(full_name)
    
    plt.tight_layout()

//Print the remaining features for future reference:
print(list(loans.columns))

//5.1 id
//Data Dictionary: "A unique [LendingClub] assigned ID for the loan listing."
loans['id'].sample(5)

loans['id'].describe()
loans.drop('id', axis=1, inplace=True)


//5.2 loan_amnt
//Data Dictionary: "The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value."
loans['loan_amnt'].describe()

plot_var('loan_amnt', 'Loan Amount', continuous=True)

//Charged-off loans tend to have higher loan amounts. Let's compare the summary statistics by loan status:
loans.groupby('loan_status')['loan_amnt'].describe()

//5.3 term
//Data Dictionary: "The number of payments on the loan. Values are in months and can be either 36 or 60."
loans['term'].value_counts(dropna=False)

//Convert term to integers.
loans['term'] = loans['term'].apply(lambda s: np.int8(s.split()[0]))
loans['term'].value_counts(normalize=True)

//Compare the charge-off rate by loan period:
loans.groupby('term')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']

//5.4 int_rate Step for Data Understanding
//Data Dictionary: "Interest Rate on the loan."
loans['int_rate'].describe()

plot_var('int_rate', 'Interest Rate', continuous=True)

//Charged-off loans tend to have much higher interest rates. Let's compare the summary statistics by loan status:
loans.groupby('loan_status')['int_rate'].describe()

//5.5 installment
//Data Dictionary: "The monthly payment owed by the borrower if the loan originates."
loans['installment'].describe()
plot_var('installment', 'Installment', continuous=True)

//Charged-off loans tend to have higher installments. Let's compare the summary statistics by loan status:
loans.groupby('loan_status')['installment'].describe()

//5.6 grade, sub_grade
//Data Dictionary for grade: "LendingClub assigned loan grade."
//Data Dictionary for sub_grade: "LendingClub assigned loan subgrade."
//What are the possible values of grade and sub_grade?
print(sorted(loans['grade'].unique()))
print(sorted(loans['sub_grade'].unique()))
loans.drop('grade', axis=1, inplace=True)
plot_var('sub_grade', 'Subgrade', continuous=False)

//5.7 emp_title
//Data Dictionary: "The job title supplied by the Borrower when applying for the loan."
loans['emp_title'].describe()

//There are too many different job titles for this feature to be useful, so we drop it.
loans.drop(labels='emp_title', axis=1, inplace=True)

//5.8 emp_length
//Data Dictionary: "Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years." The actual data does not match this description:
loans['emp_length'].value_counts(dropna=False).sort_index()

//Convert emp_length to integers:
loans['emp_length'].replace(to_replace='10+ years', value='10 years', inplace=True)
loans['emp_length'].replace('< 1 year', '0 years', inplace=True)
def emp_length_to_int(s):
    if pd.isnull(s):
        return s
    else:
        return np.int8(s.split()[0])
loans['emp_length'] = loans['emp_length'].apply(emp_length_to_int)
loans['emp_length'].value_counts(dropna=False).sort_index()

plot_var('emp_length', 'Employment Length', continuous=False)

//5.9 home_ownership
//Data Dictionary: "The home ownership status provided by the borrower during registration or obtained from the credit report. Our values are: RENT, OWN, MORTGAGE, OTHER."
loans['home_ownership'].value_counts(dropna=False)
loans['home_ownership'].replace(['NONE', 'ANY'], 'OTHER', inplace=True)
loans['home_ownership'].value_counts(dropna=False)
plot_var('home_ownership', 'Home Ownership', continuous=False)

//charge-off rates
loans.groupby('home_ownership')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']

//5.10 annual_inc
//Data Dictionary: "The self-reported annual income provided by the borrower during registration."
loans['annual_inc'].describe()
loans['log_annual_inc'] = loans['annual_inc'].apply(lambda x: np.log10(x+1))
loans.drop('annual_inc', axis=1, inplace=True)
loans['log_annual_inc'].describe()

plot_var('log_annual_inc', 'Log Annual Income', continuous=True)
loans.groupby('loan_status')['log_annual_inc'].describe()

//5.11 verification_status
//Data Dictionary: "Indicates if income was verified by [Lending Club], not verified, or if the income source was verified."
plot_var('verification_status', 'Verification Status', continuous=False)

//5.12 issue_d
//Data Dictionary: "The month which the loan was funded."
//Because we're only using variables available to investors before the loan was funded, issue_d will not be included in the final model. We're keeping it for now just to perform the train/test split later, then we'll drop it.


//5.13 purpose
//Data Dictionary: "A category provided by the borrower for the loan request."
loans['purpose'].value_counts()
//Calculate the charge-off rates by purpose:
loans.groupby('purpose')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off'].sort_values()

//5.14 title
//Data Dictionary: "The loan title provided by the borrower."
loans['title'].describe()

//View the top 10 loan titles, and their frequencies:
loans['title'].value_counts().head(10)

loans.drop('title', axis=1, inplace=True)

//5.15 zip_code, addr_state
//Data Dictionary for zip_code: "The first 3 numbers of the zip code provided by the borrower in the loan application."

//Data Dictionary for addr_state: "The state provided by the borrower in the loan application."
loans['zip_code'].sample(5)

loans['zip_code'].nunique()
loans['addr_state'].sample(5)
loans['addr_state'].nunique()

//There are a lot of different zip codes, so let's just keep the state column.
loans.drop(labels='zip_code', axis=1, inplace=True)

//Calculate the charge-off rates by address state:
loans.groupby('addr_state')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off'].sort_values()


//5.16 dti
//Data Dictionary: "A ratio calculated using the borrower’s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower’s self-reported monthly income."
loans['dti'].describe()
//There are several outliers that mess up our default plots. Plot a histogram for dti less than 60:
plt.figure(figsize=(8,3), dpi=90)
sns.distplot(loans.loc[loans['dti'].notnull() & (loans['dti']<60), 'dti'], kde=False)
plt.xlabel('Debt-to-income Ratio')
plt.ylabel('Count')
plt.title('Debt-to-income Ratio')

//dti outlier values
(loans['dti']>=60).sum()

//Compare the summary statistics by loan status:
loans.groupby('loan_status')['dti'].describe()

//5.17 earliest_cr_line
//Data Dictionary: "The month the borrower's earliest reported credit line was opened."
loans['earliest_cr_line'].sample(5)
loans['earliest_cr_line'].isnull().any()

loans['earliest_cr_line'] = loans['earliest_cr_line'].apply(lambda s: int(s[-4:]))
loans['earliest_cr_line'].describe()

plot_var('earliest_cr_line', 'Year of Earliest Credit Line', continuous=True)

//5.18 fico_range_low, fico_range_high
//Data Dictionary for fico_range_low: "The lower boundary range the borrower’s FICO at loan origination belongs to."
//Data Dictionary for fico_range_high: "The upper boundary range the borrower’s FICO at loan origination belongs to
loans[['fico_range_low', 'fico_range_high']].describe()

//Check the Pearson correlation between these values:
loans[['fico_range_low','fico_range_high']].corr()

//We only need to keep one of the FICO scores. We'll take the average of the two and call it fico_score:
loans['fico_score'] = 0.5*loans['fico_range_low'] + 0.5*loans['fico_range_high']

loans.drop(['fico_range_high', 'fico_range_low'], axis=1, inplace=True)
plot_var('fico_score', 'FICO Score', continuous=True)

//There is a noticeable difference in FICO scores between fully paid and charged-off loans. Compare the summary statistics:
loans.groupby('loan_status')['fico_score'].describe()


//5.19 open_acc
//Data Dictionary: "The number of open credit lines in the borrower's credit file."
plt.figure(figsize=(10,3), dpi=90)
sns.countplot(loans['open_acc'], order=sorted(loans['open_acc'].unique()), color='#5975A4', saturation=1)
_, _ = plt.xticks(np.arange(0, 90, 5), np.arange(0, 90, 5))
plt.title('Number of Open Credit Lines')

loans.groupby('loan_status')['open_acc'].describe()

//5.20 pub_rec
//Data Dictionary: "Number of derogatory public records."
loans['pub_rec'].value_counts().sort_index()

loans.groupby('loan_status')['pub_rec'].describe()

//5.21 revol_bal
//Data Dictionary: "Total credit revolving balance."
loans['revol_bal'].describe()

//Do a log transform:
loans['log_revol_bal'] = loans['revol_bal'].apply(lambda x: np.log10(x+1))
loans.drop('revol_bal', axis=1, inplace=True)
plot_var('log_revol_bal', 'Log Revolving Credit Balance', continuous=True)
loans.groupby('loan_status')['log_revol_bal'].describe()

//5.22 revol_util
//Data Dictionary: "Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit."
loans['revol_util'].describe()
plot_var('revol_util', 'Revolving Line Utilization', continuous=True)
loans.groupby('loan_status')['revol_util'].describe()

//5.23 total_acc
//Data Dictionary: "The total number of credit lines currently in the borrower's credit file."
plt.figure(figsize=(12,3), dpi=90)
sns.countplot(loans['total_acc'], order=sorted(loans['total_acc'].unique()), color='#5975A4', saturation=1)
_, _ = plt.xticks(np.arange(0, 176, 10), np.arange(0, 176, 10))
plt.title('Total Number of Credit Lines')
loans.groupby('loan_status')['total_acc'].describe()

//5.24 initial_list_status
//Data Dictionary: "The initial listing status of the loan. Possible values are – W, F." I'm not sure what this means.
plot_var('initial_list_status', 'Initial List Status', continuous=False)

//5.25 application_type
//Data Dictionary: "Indicates whether the loan is an individual application or a joint application with two co-borrowers."
loans['application_type'].value_counts()
//compare the charge-off rates by application type:
loans.groupby('application_type')['loan_status'].value_counts(normalize=True).loc[:,'Charged Off']

//5.26 mort_acc
//Data Dictionary: "Number of mortgage accounts."
loans['mort_acc'].describe()
//Check the top 10 values:

//Compare the summary statistics by loan status:
loans.groupby('loan_status')['mort_acc'].describe()

//5.27 pub_rec_bankruptcies
//Data Dictionary: "Number of public record bankruptcies."
loans['pub_rec_bankruptcies'].value_counts().sort_index()
plot_var('pub_rec_bankruptcies', 'Public Record Bankruptcies', continuous=False)


//6. More Pre-processing

//6.1 Convert loan status to 0/1 charge-off indicator
//Change the response variable loan_status to a 0/1 variable, where 0 indicates fully paid and 1 indicates ch
loans['charged_off'] = (loans['loan_status'] == 'Charged Off').apply(np.uint8)
loans.drop('loan_status', axis=1, inplace=True)

//6.2 Create dummy variables
loans.shape

//If any categorical variables have missing values, we'll need to create NaN dummy variables for those. So first check which variables h
missing_fractions = loans.isnull().mean().sort_values(ascending=False) # Fraction of data missing for each variable
print(missing_fractions[missing_fractions > 0]) # Print variables that are missing data

//Create dummy variables for the categorical variables:
print(loans.columns)
loans = pd.get_dummies(loans, columns=['sub_grade', 'home_ownership', 'verification_status', 'purpose', 'addr_state', 'initial_list_status', 'application_type'], drop_first=True)
loans.shape
//Check our data with the new dummy variables:
loans.sample(5)

//6.3 Train/test split
//The variable issue_d includes the month and year that the loan was funded.
loans['issue_d'].sample(5)
loans['issue_d'].isnull().any()
//convert the issue dates to datetime objects:
loans['issue_d'] = pd.to_datetime(loans['issue_d'])
loans['issue_d'].sample(5)

//The new datetime values are all on the first day of the month. Check the summary statistics of the issue dates:
loans['issue_d'].describe()
plt.figure(figsize=(6,3), dpi=90)
loans['issue_d'].dt.year.value_counts().sort_index().plot.bar(color='darkblue')
plt.xlabel('Year')
plt.ylabel('Number of Loans Funded')
plt.title('Loans Funded per Year')

loans_train = loans.loc[loans['issue_d'] <  loans['issue_d'].quantile(0.9)]
loans_test =  loans.loc[loans['issue_d'] >= loans['issue_d'].quantile(0.9)]

print('Number of loans in the partition:   ', loans_train.shape[0] + loans_test.shape[0])
print('Number of loans in the full dataset:', loans.shape[0])
loans_test.shape[0] / loans.shape[0]

//delete the original loans dataframe:
del loans

//summary statistics of the issue dates in the train and test sets:
loans_train['issue_d'].describe()
loans_test['issue_d'].describe()

loans_train.drop('issue_d', axis=1, inplace=True)
loans_test.drop('issue_d', axis=1, inplace=True)

subsetloans_train=loans_train.sample(frac=0.1)

#y_train = loans_train['charged_off']
y_train = subsetloans_train['charged_off']

y_test = loans_test['charged_off']

#X_train = loans_train.drop('charged_off', axis=1)
X_train = subsetloans_train.drop('charged_off', axis=1)
X_test = loans_test.drop('charged_off', axis=1)

del loans_train, loans_test


///7. Linear Dependence of Charge-off on the Predictors
linear_dep = pd.DataFrame()
//Pearson correlations:
for col in X_train.columns:
    linear_dep.loc[col, 'pearson_corr'] = X_train[col].corr(y_train)
linear_dep['abs_pearson_corr'] = abs(linear_dep['pearson_corr'])

from sklearn.feature_selection import f_classif
for col in X_train.columns:
    mask = X_train[col].notnull()
    (linear_dep.loc[col, 'F'], linear_dep.loc[col, 'p_value']) = f_classif(pd.DataFrame(X_train.loc[mask, col]), y_train.loc[mask])

//Sort the results by the absolute value of the Pearson correlation:
linear_dep.sort_values('abs_pearson_corr', ascending=False, inplace=True)
linear_dep.drop('abs_pearson_corr', axis=1, inplace=True)

//Reset the index:
linear_dep.reset_index(inplace=True)
linear_dep.rename(columns={'index':'variable'}, inplace=True)

//View the results for the top 20 predictors most correlated with charged_off:
linear_dep.head(20)

//Now view the results for the 20 least correlated predictors:
linear_dep.tail(20)


//8. Model Training and Testing
/*We implement machine learning pipelines consisting of one or more of the following steps, depending on the particular model:

Mean imputation of missing values
Dimension reduction using linear discriminant analysis (LDA)
Data standardization: rescaling to zero mean and unit variance
The chosen model

We will evaluate and compare the following models using a cross-validated AUROC score on the training set:
Logistic regression with SGD training
Random forest
k-nearest neighbors
We'll perform some hyperparameter tuning for each model to choose the most promising model, then more carefully tune the hyperparameters of the best-performing model.*/
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import GridSearchCV

//8.1 Logistic regression with SGD training
//The SGDClassifier estimator in scikit-learn implements linear classifiers (SVM, logistic regression, and others) with stochastic gradient descent (SGD) training. A particular linear classifier is chosen through the loss hyperparameter. Because we want to predict the probability of charge-off, we choose logistic regression (a probabilistic classifier) by setting loss = 'log'.

from sklearn.linear_model import SGDClassifier
//The machine learning pipeline:
pipeline_sgdlogreg = Pipeline([
    ('imputer', SimpleImputer(copy=False)), # Mean imputation by default
    ('scaler', StandardScaler(copy=False)),
    ('model', SGDClassifier(loss='log', max_iter=1000, tol=1e-3, random_state=1, warm_start=True))
])
pipeline_sgdlogreg.get_params().keys()

//A small grid of hyperparameters to search over:
param_grid_sgdlogreg = {
    'model__alpha': [10**-5, 10**-2, 10**1],
    'model__penalty': ['l1', 'l2']
    #,'model__max_iter': [100,200,300]
}

//Create the search grid object:
grid_sgdlogreg = GridSearchCV(estimator=pipeline_sgdlogreg, param_grid=param_grid_sgdlogreg, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)

//Conduct the grid search and train the final model on the whole dataset:
grid_sgdlogreg.fit(X_train, y_train)

//Mean cross-validated AUROC score of the best model:
grid_sgdlogreg.best_score_

//Best hyperparameters:
grid_sgdlogreg.best_params_

//8.2 Support Vector Machine with SGD training. Dantong Provides an example on how to introduce a new machine learning model (Support Vector Machine or Neural Networks) into the script. You can customize your code to get a better performance. You can use GridSearch Cross Valide to find the best parameter set.

from sklearn.svm import SVC  
pipeline_SVM = Pipeline([
    ('imputer', SimpleImputer(copy=False)),
    ('scaler', StandardScaler(copy=False)),
    ('svc', SVC())
   ##
])

pipeline_SVM.get_params().keys()
param_grid_SVM = {
    'svc__kernel': ['rbf','linear'], 
    'svc__gamma':[0.1,0.01]
    #,'svc__C': [1, 10, 100]
                 }

# You can use the GridSearchCV over pipeline to get the best set of hyper-parameters! 
#pipeline_SVM.classes_
grid_SVM = GridSearchCV(estimator=pipeline_SVM, param_grid=param_grid_SVM, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=2, verbose=1, return_train_score=False)


grid_SVM.fit(X_train, y_train)
grid_SVM.best_score_
grid_SVM.best_params_

//8.3 Random forest classifier
//Next we train a random forest model. Note that data standardization is not necessary for a random forest.
from sklearn.ensemble import RandomForestClassifier
pipeline_rfc = Pipeline([
    ('imputer', SimpleImputer(copy=False)),
    ('model', RandomForestClassifier(n_jobs=-1, random_state=10))
])


pipeline_rfc.get_params().keys()
//The random forest takes very long to train, so we don't test different hyperparameter choices. We'll still use GridSearchCV for the sake of consistency.
param_grid_rfc = {
    'model__n_estimators': [60] 
    
}
//The AUROC will always improve (with decreasing gains) as the number of estimators increases, but it's not necessarily worth the extra training time and model complexity.
rid_rfc = grid_rfc = GridSearchCV(estimator=pipeline_rfc, param_grid=param_grid_rfc, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)
grid_rfc.fit(X_train, y_train)

//Mean cross-validated AUROC score of the random forest:
grid_rfc.best_score_

//8.4 k-nearest neighbors
from sklearn.neighbors import KNeighborsClassifier
pipeline_knn = Pipeline([
    ('imputer', SimpleImputer(copy=False)),
    ('scaler', StandardScaler(copy=False)),
    ('lda', LinearDiscriminantAnalysis()),
    ('model', KNeighborsClassifier(n_jobs=-1))
])
pipeline_knn.get_params().keys()
param_grid_knn = {
    #'lda__n_components': [1], #Nan with anything >1 
    'model__n_neighbors': [10, 20, 80, 160],
    'model__n_jobs': [-1],
}
grid_knn = GridSearchCV(estimator=pipeline_knn, param_grid=param_grid_knn, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)
grid_knn.fit(X_train, y_train)
Mean cross-validated AUROC score of the best model:
grid_knn.best_score_

//Best hyperparameters:
grid_knn.best_params_

//Neural Network
from sklearn.neural_network import MLPClassifier
pipeline_nn = Pipeline([
    ('imputer', SimpleImputer(copy=False)),
    ('scaler', StandardScaler(copy=False)),
    ('model', MLPClassifier())
])
pipeline_nn.get_params().keys()
param_grid_nn = {
    'model__max_iter': [400],
    'model__tol':[1e-4]
    #,'model__hidden_layer_sizes':[2,5,10]
    ,'model__hidden_layer_sizes':range(2,5)
}
grid_nn = GridSearchCV(estimator=pipeline_nn, param_grid=param_grid_nn, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)
grid_nn.fit(X_train, y_train)
grid_nn.best_score_
grid_nn.best_params_

//8.5 Tune hyperparameters on the chosen model more finely
//The three models performed quite similarly according to the AUROC:
print('Cross-validated AUROC scores')
print(grid_sgdlogreg.best_score_, '- Logistic regression')
print(grid_rfc.best_score_, '- Random forest')
print(grid_knn.best_score_, '- k-nearest neighbors')
print(grid_nn.best_score_, '- Nueral Network')
print(grid_SVM.best_score_, '- SVM')
//Logistic regression squeaked out ahead, and coupled with the fact that SGDClassifier trains much faster than the other two models, we'll select logistic regression as our final model. Now we'll tune the hyperparameters more finely.

param_grid_sgdlogreg = {
    'model__alpha': [10**-5, 10**-2, 10**1],
    #'model__penalty': ['l1', 'l2']
    'model__penalty': ['l2']
}
grid_sgdlogreg = GridSearchCV(estimator=pipeline_sgdlogreg, param_grid=param_grid_sgdlogreg, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)
grid_sgdlogreg.fit(X_train, y_train)

//Mean cross-validated AUROC score of the best model:
grid_sgdlogreg.best_score_

//Best hyperparameters:
grid_sgdlogreg.best_params_

//8.6 Test set evaluation
from sklearn.metrics import roc_auc_score
y_score = grid_sgdlogreg.predict_proba(X_test)[:,1]
roc_auc_score(y_test, y_score)

y_score = grid_SVM.predict_proba(X_test)[:,1]
roc_auc_score(y_test, y_score)

y_score = grid_SVM.predict(X_test)
print(y_score)
roc_auc_score(y_test, y_score)

y_score = grid_knn.predict_proba(X_test)[:,1]
roc_auc_score(y_test, y_score)

y_score = grid_rfc.predict_proba(X_test)[:,1]
roc_auc_score(y_test, y_score)

//The test set AUROC score is somewhat lower than the cross-validated score (0.713)
